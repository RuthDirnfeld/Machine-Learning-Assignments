{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from scipy.stats import pearsonr\n",
    "import itertools\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.2.1 Load the data and get an overview of the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ISLR/data/Boston.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "1. 'crim' 2. 'zn' 3. 'indus' 4. 'chas' 5. 'nox' 6. 'rm' 7. 'age' 8. 'dis' 9. 'rad' 10. 'tax' 11. 'ptratio' 12. 'black' 13. 'lstat' 14. 'medv' "
     ]
    }
   ],
   "source": [
    "r, c = df.shape\n",
    "print(c)\n",
    "for a, b in enumerate(df, 1):\n",
    "    list = \"\".join('{}. \\'{}\\''.format(a, b))\n",
    "    print (list, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   medv   R-squared:                       0.708\n",
      "Model:                            OLS   Adj. R-squared:                  0.705\n",
      "Method:                 Least Squares   F-statistic:                     242.6\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          3.67e-131\n",
      "Time:                        19:07:31   Log-Likelihood:                -1528.7\n",
      "No. Observations:                 506   AIC:                             3069.\n",
      "Df Residuals:                     500   BIC:                             3095.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     37.4992      4.613      8.129      0.000      28.436      46.562\n",
      "lstat         -0.5811      0.048    -12.122      0.000      -0.675      -0.487\n",
      "rm             4.1633      0.412     10.104      0.000       3.354       4.973\n",
      "nox          -17.9966      3.261     -5.519      0.000     -24.403     -11.590\n",
      "dis           -1.1847      0.168     -7.034      0.000      -1.516      -0.854\n",
      "ptratio       -1.0458      0.114     -9.212      0.000      -1.269      -0.823\n",
      "==============================================================================\n",
      "Omnibus:                      187.456   Durbin-Watson:                   0.971\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              885.498\n",
      "Skew:                           1.584   Prob(JB):                    5.21e-193\n",
      "Kurtosis:                       8.654   Cond. No.                         545.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "result1 = sm.OLS.from_formula('medv ~ lstat + rm + nox + dis + ptratio', df).fit()\n",
    "print(result1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   medv   R-squared:                       0.778\n",
      "Model:                            OLS   Adj. R-squared:                  0.775\n",
      "Method:                 Least Squares   F-statistic:                     290.8\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          2.48e-159\n",
      "Time:                        19:07:33   Log-Likelihood:                -1459.9\n",
      "No. Observations:                 506   AIC:                             2934.\n",
      "Df Residuals:                     499   BIC:                             2963.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      3.1518      4.880      0.646      0.519      -6.435      12.739\n",
      "lstat          1.8115      0.196      9.237      0.000       1.426       2.197\n",
      "rm             8.3344      0.491     16.971      0.000       7.370       9.299\n",
      "lstat:rm      -0.4185      0.034    -12.488      0.000      -0.484      -0.353\n",
      "nox          -12.3651      2.885     -4.286      0.000     -18.033      -6.697\n",
      "dis           -1.0184      0.148     -6.893      0.000      -1.309      -0.728\n",
      "ptratio       -0.7152      0.103     -6.967      0.000      -0.917      -0.514\n",
      "==============================================================================\n",
      "Omnibus:                      246.928   Durbin-Watson:                   1.079\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2792.613\n",
      "Skew:                           1.836   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.908   Cond. No.                     2.36e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.36e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "result2 = sm.OLS.from_formula('medv ~ lstat * rm + nox + dis + ptratio', df).fit()\n",
    "print(result2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interpretation</h2>\n",
    "\n",
    "The syntax lstat * rm simultaneously includes lstat, rm, and the interaction term lstat × rm as predictors; it is a shorthand for lstat + rm + lstat : rm, as can be seen in the results table as well.  \n",
    "\n",
    "**lstat : rm**  \n",
    "To the left of the ~ is the dependent variable (medv). After the ~, we list the two predictor variables. The * indicates that not only do we want each main effect, but we also want an interaction term between lstat and rm.  \n",
    "From the results table we see, that individually lstat and rm influence the house price positively, but the interaction lstat:rm negatively.\n",
    "\n",
    "The R-squared value, which is 77.8% tells us that this model is highly significant.\n",
    "What can be noticed in the summary is the fact that the features nox, dis, ptratio and lstat:rm have negative values, which means that they are not only significant, but also have a relationship to lowering the price of the house.  \n",
    "Thus, the conclusion is that the features nox, dis, ptratio and lstat:rm have a relation to lower house prices. Furthermore, the features with negative values, also have a highly significant p-value, which means that it is unlikely that there is no relationship between medv and the given feature.  \n",
    "\n",
    "On the other hand, the following features have positive values: lstat and rm.  \n",
    "The conclusion for this is, that for example the houses with more rooms have a relationship with increasing the house value. The p-value for rm for example is also highly significant, which also means that it is unlikely that there is no relationship between the house value and the number of rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   medv   R-squared:                       0.781\n",
      "Model:                            OLS   Adj. R-squared:                  0.778\n",
      "Method:                 Least Squares   F-statistic:                     253.9\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          8.05e-160\n",
      "Time:                        19:07:58   Log-Likelihood:                -1455.8\n",
      "No. Observations:                 506   AIC:                             2928.\n",
      "Df Residuals:                     498   BIC:                             2961.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                10.5522      5.499      1.919      0.056      -0.253      21.357\n",
      "lstat                     1.5468      0.216      7.167      0.000       1.123       1.971\n",
      "rm                        7.6004      0.552     13.777      0.000       6.516       8.684\n",
      "lstat:rm                 -0.4468      0.035    -12.864      0.000      -0.515      -0.379\n",
      "np.square(lstat * rm)     0.0004      0.000      2.845      0.005       0.000       0.001\n",
      "nox                     -12.2898      2.865     -4.290      0.000     -17.918      -6.662\n",
      "dis                      -1.0641      0.148     -7.209      0.000      -1.354      -0.774\n",
      "ptratio                  -0.7112      0.102     -6.977      0.000      -0.912      -0.511\n",
      "==============================================================================\n",
      "Omnibus:                      217.415   Durbin-Watson:                   1.059\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2007.945\n",
      "Skew:                           1.622   Prob(JB):                         0.00\n",
      "Kurtosis:                      12.204   Cond. No.                     3.02e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.02e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "result3 = sm.OLS.from_formula('medv ~ lstat * rm + np.square(lstat * rm) + nox + dis + ptratio', df).fit()\n",
    "print(result3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   df_resid          ssr  df_diff     ss_diff         F    Pr(>F)\n",
      "0     499.0  9500.381881      0.0         NaN       NaN       NaN\n",
      "1     498.0  9348.435955      1.0  151.945925  8.094303  0.004623\n"
     ]
    }
   ],
   "source": [
    "print(sm.stats.anova_lm(result2, result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>General</h3>\n",
    "Most of the results can be tracked back to the summaries as differences, such as the difference in the degrees of freedom, where in this case the difference is 1, since first model has 6 and the second model has 7 degrees of freedom. Thus the difference is one. df_resid shows the Df Residuals of both models, where the first model has  499.0 and the second one has 498.0. \n",
    "For the F-values in this example, and also in all following anova tests, if the value was close to 1, it would mean that there is no relationship between the response and the predictors. If the value is greater than 1 it means that there is a relationship.  \n",
    "\n",
    "<h2>Interpretation</h2>\n",
    "\n",
    "The increase of R2 and the low p-value associated with the quadratic term suggests that it leads\n",
    "to an improved model. The ANOVA test was performed to check whether the quadratic fit is superior to the linear fit. As it is visible, the result shows a degree of freedom difference of 1 (indicating that the more complex model has one additional parameter), and very small p-value. Higher degrees of freedom generally mean larger sample sizes, a higher degree of freedom means more power to reject a false null hypothesis and find a significant result. Furthermore, since the p-value is 0.0046, which is < 0.05, we reject the null hypothesis and conclude that the model which squares lstat * rm (uses np.square(lstat * rm)), thus the result3, is significantly better than the model which uses lstat * rm, result2. So, in this case, we use model result3 as a model preferred over result2. Thus, the conclusion is, that the quadratic fit is superior to the linear fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the way polynomials actually work, however, this gives a weird coefficients output\n",
    "result4 = sm.OLS.from_formula('medv ~ lstat * rm +' + '+'.join(['np.power(lstat,' + str(i) + ')' for i in range(1,6)]) + '+ nox + dis + ptratio', df).fit()\n",
    "#print(result4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   medv   R-squared:                       0.792\n",
      "Model:                            OLS   Adj. R-squared:                  0.787\n",
      "Method:                 Least Squares   F-statistic:                     188.0\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          1.80e-161\n",
      "Time:                        20:02:33   Log-Likelihood:                -1443.5\n",
      "No. Observations:                 506   AIC:                             2909.\n",
      "Df Residuals:                     495   BIC:                             2956.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            15.7971      5.896      2.679      0.008       4.213      27.381\n",
      "lstat                 1.1281      0.309      3.650      0.000       0.521       1.735\n",
      "rm                    6.5291      0.711      9.183      0.000       5.132       7.926\n",
      "lstat:rm             -0.3055      0.052     -5.878      0.000      -0.408      -0.203\n",
      "poly(lstat, 5)[0]    -1.2385      0.466     -2.656      0.008      -2.155      -0.322\n",
      "poly(lstat, 5)[1]   -17.1654      6.989     -2.456      0.014     -30.896      -3.435\n",
      "poly(lstat, 5)[2]   -13.8101      4.592     -3.008      0.003     -22.832      -4.788\n",
      "poly(lstat, 5)[3]    13.8401      4.530      3.055      0.002       4.939      22.741\n",
      "poly(lstat, 5)[4]    15.0936      4.295      3.514      0.000       6.655      23.533\n",
      "nox                 -13.7513      2.823     -4.871      0.000     -19.298      -8.204\n",
      "dis                  -1.0326      0.145     -7.127      0.000      -1.317      -0.748\n",
      "ptratio              -0.7407      0.101     -7.324      0.000      -0.939      -0.542\n",
      "==============================================================================\n",
      "Omnibus:                      232.049   Durbin-Watson:                   1.116\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2275.267\n",
      "Skew:                           1.742   Prob(JB):                         0.00\n",
      "Kurtosis:                      12.787   Cond. No.                     3.53e+18\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.2e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#most similar output to the R example:\n",
    "poly = lambda x, degree : np.linalg.qr(np.vander(x, degree + 1)[:, ::-1])[0][:, 1:]\n",
    "\n",
    "result4 = sm.OLS.from_formula('medv ~ lstat * rm + poly(lstat,5) + nox + dis + ptratio', df).fit()\n",
    "print(result4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   df_resid          ssr  df_diff     ss_diff         F    Pr(>F)\n",
      "0     499.0  9500.381881      0.0         NaN       NaN       NaN\n",
      "1     495.0  8903.772453      4.0  596.609428  8.292038  0.000002\n"
     ]
    }
   ],
   "source": [
    "print(sm.stats.anova_lm(result2, result4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interpretation</h2>\n",
    "\n",
    "The increase of R2 and the low p-value associated with the polynomial term suggests that it leads\n",
    "to an improved model. The ANOVA test was performed to check whether the polynomial fit is superior to the linear fit. As it is visible, the result shows a degree of freedom difference of 4 (indicating that the more complex model has four additional parameters), and very small p-value. Higher degrees of freedom generally mean larger sample sizes, a higher degree of freedom means more power to reject a false null hypothesis and find a significant result. Furthermore, since the p-value is 0.000002, which is < 0.05, we reject the null hypothesis and conclude that the model polynomial up to 5, thus the result4, is significantly better than the model which uses lstat * rm, result2. So, in this case, we use model result4 as a model preferred over result2. Thus, the conclusion is, that the polynomial fit is superior to the linear fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   medv   R-squared:                       0.804\n",
      "Model:                            OLS   Adj. R-squared:                  0.800\n",
      "Method:                 Least Squares   F-statistic:                     202.6\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          7.10e-168\n",
      "Time:                        18:19:25   Log-Likelihood:                -1428.4\n",
      "No. Observations:                 506   AIC:                             2879.\n",
      "Df Residuals:                     495   BIC:                             2925.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept           143.2756     14.094     10.166      0.000     115.584     170.967\n",
      "poly(lstat, 5)[0]  -105.2242      6.667    -15.782      0.000    -118.324     -92.125\n",
      "poly(lstat, 5)[1]   -29.8131      4.938     -6.038      0.000     -39.515     -20.111\n",
      "poly(lstat, 5)[2]   -11.4584      4.445     -2.578      0.010     -20.192      -2.725\n",
      "poly(lstat, 5)[3]    15.9352      4.260      3.740      0.000       7.564      24.306\n",
      "poly(lstat, 5)[4]    17.9448      4.151      4.323      0.000       9.789      26.101\n",
      "rm                   25.1967      2.732      9.224      0.000      19.830      30.564\n",
      "np.log(rm)         -137.4038     16.761     -8.198      0.000    -170.336    -104.472\n",
      "nox                 -16.6408      2.734     -6.087      0.000     -22.012     -11.270\n",
      "dis                  -0.9709      0.141     -6.885      0.000      -1.248      -0.694\n",
      "ptratio              -0.7843      0.097     -8.116      0.000      -0.974      -0.594\n",
      "==============================================================================\n",
      "Omnibus:                      221.958   Durbin-Watson:                   1.064\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2718.500\n",
      "Skew:                           1.567   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.914   Cond. No.                     2.41e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.41e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      " Anova Test: \n",
      " \n",
      "   df_resid          ssr  df_diff     ss_diff          F        Pr(>F)\n",
      "0     499.0  9500.381881      0.0         NaN        NaN           NaN\n",
      "1     495.0  8386.756361      4.0  1113.62552  16.431997  1.190966e-12\n"
     ]
    }
   ],
   "source": [
    "result5 = sm.OLS.from_formula('medv ~ poly(lstat,5) + rm + np.log(rm) + nox + dis + ptratio',df).fit()\n",
    "print(result5.summary())\n",
    "print('\\n Anova Test: \\n ')\n",
    "print(sm.stats.anova_lm(result2, result5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interpretation</h2>\n",
    "\n",
    "The increase of R2 and the low p-value associated with the other non-linear transofrmations, such as log(X) suggests that it leads to an improved model. The ANOVA test was performed to check whether the log(X) fit is superior to the linear fit. As it is visible, the result shows a degree of freedom difference of 4 (indicating that the more complex model has four additional parameters), and very small p-value. Higher degrees of freedom generally mean larger sample sizes, a higher degree of freedom means more power to reject a false null hypothesis and find a significant result. Furthermore, since the p-value is 0.000000000001190966, which is < 0.05, we reject the null hypothesis and conclude that the model which uses log(X), thus the result5, is significantly better than the model which uses lstat * rm, result2. So, in this case, we use model result5 as a model preferred over result2. Thus, the conclusion is, that the other non-linear transofrmations, such as log(X) fit is superior to the linear fit.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   medv   R-squared:                       0.806\n",
      "Model:                            OLS   Adj. R-squared:                  0.802\n",
      "Method:                 Least Squares   F-statistic:                     186.6\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          5.56e-168\n",
      "Time:                        18:20:08   Log-Likelihood:                -1425.4\n",
      "No. Observations:                 506   AIC:                             2875.\n",
      "Df Residuals:                     494   BIC:                             2925.\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept               124.7429     15.961      7.815      0.000      93.383     156.103\n",
      "poly(lstat, 5)[0]       -39.3464     27.890     -1.411      0.159     -94.144      15.451\n",
      "poly(lstat, 5)[1]       -37.9023      5.934     -6.388      0.000     -49.561     -26.244\n",
      "poly(lstat, 5)[2]       -15.8957      4.785     -3.322      0.001     -25.297      -6.495\n",
      "poly(lstat, 5)[3]        13.4255      4.363      3.077      0.002       4.853      21.998\n",
      "poly(lstat, 5)[4]        17.6791      4.132      4.278      0.000       9.560      25.798\n",
      "rm                       23.1268      2.848      8.120      0.000      17.531      28.723\n",
      "np.log(rm)             -119.2994     18.265     -6.532      0.000    -155.186     -83.413\n",
      "np.square(lstat * rm)    -0.0004      0.000     -2.432      0.015      -0.001   -7.44e-05\n",
      "nox                     -15.8501      2.740     -5.786      0.000     -21.233     -10.467\n",
      "dis                      -0.9507      0.141     -6.764      0.000      -1.227      -0.675\n",
      "ptratio                  -0.7465      0.097     -7.664      0.000      -0.938      -0.555\n",
      "==============================================================================\n",
      "Omnibus:                      231.760   Durbin-Watson:                   1.080\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3095.007\n",
      "Skew:                           1.630   Prob(JB):                         0.00\n",
      "Kurtosis:                      14.669   Cond. No.                     1.76e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.76e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      " Anova Test: \n",
      " \n",
      "   df_resid          ssr  df_diff      ss_diff          F        Pr(>F)\n",
      "0     499.0  9500.381881      0.0          NaN        NaN           NaN\n",
      "1     494.0  8287.540755      5.0  1212.841125  14.458898  3.124012e-13\n"
     ]
    }
   ],
   "source": [
    "beat_teacher = sm.OLS.from_formula('medv ~ poly(lstat,5) + rm + np.log(rm) + np.square(lstat * rm) + nox + dis + ptratio',df).fit()\n",
    "print(beat_teacher.summary())\n",
    "print('\\n Anova Test: \\n ')\n",
    "print(sm.stats.anova_lm(result2, beat_teacher))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Beat the teacher</h2>\n",
    "\n",
    "The increase of R2 (from teacher's version of 80.4\\% to this version 80.6\\%) and the low p-value associated with the adding of the np.square(lstat * rm) suggests that it leads to an improved model. The ANOVA test was performed to check whether the new fit is superior to the linear fit. As it is visible, the result shows a degree of freedom difference of 5 (indicating that the more complex model has five additional parameters), and very small p-value. Higher degrees of freedom generally mean larger sample sizes, a higher degree of freedom means more power to reject a false null hypothesis and find a significant result. Furthermore, since the p-value is 0.0000000000003124012, which is < 0.05, we reject the null hypothesis and conclude that the model with the polynomial and np.square(lstat * rm), thus the beat_teacher, is significantly better than the model which uses lstat * rm, result2. So, in this case, we use model beat_teacher as a model preferred over result2. Thus, the conclusion is, that the addition of np.square(lstat * rm) to the result4 fit is even more superior to the linear fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompPrice</th>\n",
       "      <th>Income</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.496325</td>\n",
       "      <td>124.975000</td>\n",
       "      <td>68.657500</td>\n",
       "      <td>6.635000</td>\n",
       "      <td>264.840000</td>\n",
       "      <td>115.795000</td>\n",
       "      <td>53.322500</td>\n",
       "      <td>13.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.824115</td>\n",
       "      <td>15.334512</td>\n",
       "      <td>27.986037</td>\n",
       "      <td>6.650364</td>\n",
       "      <td>147.376436</td>\n",
       "      <td>23.676664</td>\n",
       "      <td>16.200297</td>\n",
       "      <td>2.620528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.390000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.490000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.320000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>398.500000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.270000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sales   CompPrice      Income  Advertising  Population  \\\n",
       "count  400.000000  400.000000  400.000000   400.000000  400.000000   \n",
       "mean     7.496325  124.975000   68.657500     6.635000  264.840000   \n",
       "std      2.824115   15.334512   27.986037     6.650364  147.376436   \n",
       "min      0.000000   77.000000   21.000000     0.000000   10.000000   \n",
       "25%      5.390000  115.000000   42.750000     0.000000  139.000000   \n",
       "50%      7.490000  125.000000   69.000000     5.000000  272.000000   \n",
       "75%      9.320000  135.000000   91.000000    12.000000  398.500000   \n",
       "max     16.270000  175.000000  120.000000    29.000000  509.000000   \n",
       "\n",
       "            Price         Age   Education  \n",
       "count  400.000000  400.000000  400.000000  \n",
       "mean   115.795000   53.322500   13.900000  \n",
       "std     23.676664   16.200297    2.620528  \n",
       "min     24.000000   25.000000   10.000000  \n",
       "25%    100.000000   39.750000   12.000000  \n",
       "50%    117.000000   54.500000   14.000000  \n",
       "75%    131.000000   66.000000   16.000000  \n",
       "max    191.000000   80.000000   18.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('ISLR/data/Carseats.csv',index_col=0)\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium    219\n",
      "Bad        96\n",
      "Good       85\n",
      "Name: ShelveLoc, dtype: int64\n",
      "\n",
      "\n",
      "Yes    282\n",
      "No     118\n",
      "Name: Urban, dtype: int64\n",
      "\n",
      "\n",
      "Yes    258\n",
      "No     142\n",
      "Name: US, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df2['ShelveLoc'].value_counts())\n",
    "print('\\n')\n",
    "print(df2['Urban'].value_counts())\n",
    "print('\\n')\n",
    "print(df2['US'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.873\n",
      "Model:                            OLS   Adj. R-squared:                  0.870\n",
      "Method:                 Least Squares   F-statistic:                     243.4\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          1.60e-166\n",
      "Time:                        18:22:34   Log-Likelihood:                -568.99\n",
      "No. Observations:                 400   AIC:                             1162.\n",
      "Df Residuals:                     388   BIC:                             1210.\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               5.6606      0.603      9.380      0.000       4.474       6.847\n",
      "ShelveLoc[T.Good]       4.8502      0.153     31.678      0.000       4.549       5.151\n",
      "ShelveLoc[T.Medium]     1.9567      0.126     15.516      0.000       1.709       2.205\n",
      "US[T.Yes]              -0.1841      0.150     -1.229      0.220      -0.479       0.111\n",
      "Urban[T.Yes]            0.1229      0.113      1.088      0.277      -0.099       0.345\n",
      "Advertising             0.1231      0.011     11.066      0.000       0.101       0.145\n",
      "Age                    -0.0460      0.003    -14.472      0.000      -0.052      -0.040\n",
      "CompPrice               0.0928      0.004     22.378      0.000       0.085       0.101\n",
      "Education              -0.0211      0.020     -1.070      0.285      -0.060       0.018\n",
      "Income                  0.0158      0.002      8.565      0.000       0.012       0.019\n",
      "Population              0.0002      0.000      0.561      0.575      -0.001       0.001\n",
      "Price                  -0.0954      0.003    -35.700      0.000      -0.101      -0.090\n",
      "==============================================================================\n",
      "Omnibus:                        0.811   Durbin-Watson:                   2.013\n",
      "Prob(Omnibus):                  0.667   Jarque-Bera (JB):                0.765\n",
      "Skew:                           0.107   Prob(JB):                        0.682\n",
      "Kurtosis:                       2.994   Cond. No.                     4.15e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.15e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "result6 = sm.OLS.from_formula('Sales ~' + '+'.join(df2.columns.difference(['Sales'])), df2).fit()\n",
    "print(result6.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interpretation</h2>\n",
    "\n",
    "From, for example, df2\\['ShelveLoc'\\].value_counts() it is visible that Shelveloc takes on three possible values (Bad, Medium, and Good). Python generates automatically dummy variables for a given feature such as Shelveloc. In this case, the generated dummy for Shelveloc are ShelveLoc\\[T.Good\\], which takes on a value of 1 if the shelving location is good, else if it is bad it takes on 0. Similarly, if the shelving location is medium, ShelveLoc\\[T.Medium\\] takes on 1, otherwise 0.  \n",
    "In the summary output it is visible that the ShelveLoc\\[T.Good\\] has a positive coefficient, which indicates that if the shelving location is good it is also associated to higher sales. The ShelveLoc\\[T.Medium\\] has also a positive coefficient, which also indicates that a medium shelving location leads to higher sales than a bad shelving location. However, the medium shelving location means lower sales than a good shelving location as well in this case.  \n",
    "Other duumy variables are US\\[T.Yes\\] and Urban\\[T.Yes\\].  \n",
    "Urban\\[T.Yes\\] has a positive coefficient, however a p-value of 0.277, which is > 0.05. This suggests that there isn’t a relationship between the location of the store and the number of sales based on the high p-value of the t-statistic. The coefficient states a positive relationship between Urban\\[T.Yes\\] and Sales: if the store is in an Urban area, the sales will increase by approximately 123 units.   \n",
    "Similarly, US\\[T.Yes\\] has a negative coefficient, however a p-value of 0.220, which is also > 0.05. This suggests that there isn’t a relationship between whether the store is in the US or not and the number of sales based on the high p-value of the t-statistic. The coefficient states a negative relationship between USYes and Sales: if the store is in the US, the sales will decrease by approximately 184 units.  \n",
    "\n",
    "**Other data**  \n",
    "The R-squared value, which is 87.3% tells us that this model is highly significant.  \n",
    "What can be noticed in the summary is that the following features have not only negative coefficients, but some of them also high p -values.  More specifically, features such as:  \n",
    "US: Whether the store is in the US (Yes/No) (p-val: 0.220)  \n",
    "Education: Education level at location (p-val: 0.285)  \n",
    "Price: Price for car seats at each site (p-val: 0.000)  \n",
    "Age: age level of the population (p-val: 0.000)  \n",
    "\n",
    "Thus, the conclusion is that features such as US and Education, which a p-value > 0.05 suggests that there isn’t a relationship between whether the store is in the US or not, or the education level at a location and the number of sales. However, all four features have a relation to lower the number of sales. The features Price and Age with negative values also have highly significant p-values, which means that it is unlikely that there is no relationship between the number of sales and the given feature.\n",
    "\n",
    "On the other hand, the following features have positive values:\n",
    "ShelveLoc\\[T.Good\\]  (p-val: 0.000)\n",
    "ShelveLoc\\[T.Medium\\] (p-val: 0.000)\n",
    "Urban\\[T.Yes\\] (p-val: 0.277)\n",
    "Advertising (p-val: 0.000)\n",
    "CompPrice (p-val: 0.000)\n",
    "Income (p-val: 0.000)\n",
    "Population (p-val: 0.575)\n",
    "\n",
    "By looking at the individual p-values, it is visible that there isn’t a relationship between the location of the store (Urban\\[T.Yes\\]) or regional pop in thousands (Population) and the number of sales.  \n",
    "Furthermore, good or medium shelve location, local ad budget at each location in 1000s of dollars (Advertising), price charged by competitor at each location (CompPrice), and community income level in 1000s of dollars (Income) all increase the number of sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.870\n",
      "Model:                            OLS   Adj. R-squared:                  0.868\n",
      "Method:                 Least Squares   F-statistic:                     328.2\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          2.90e-168\n",
      "Time:                        18:37:44   Log-Likelihood:                -573.74\n",
      "No. Observations:                 400   AIC:                             1165.\n",
      "Df Residuals:                     391   BIC:                             1201.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               3.2991      0.476      6.932      0.000       2.363       4.235\n",
      "ShelveLoc[T.Good]       4.8949      0.154     31.825      0.000       4.592       5.197\n",
      "ShelveLoc[T.Medium]     1.9908      0.127     15.736      0.000       1.742       2.240\n",
      "Advertising             0.0534      0.021      2.544      0.011       0.012       0.095\n",
      "CompPrice               0.0934      0.004     22.492      0.000       0.085       0.102\n",
      "Income                  0.0098      0.003      3.756      0.000       0.005       0.015\n",
      "Price                  -0.0759      0.003    -25.591      0.000      -0.082      -0.070\n",
      "Income:Advertising      0.0009      0.000      3.124      0.002       0.000       0.001\n",
      "Price:Age              -0.0004   2.69e-05    -13.713      0.000      -0.000      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                        1.537   Durbin-Watson:                   1.988\n",
      "Prob(Omnibus):                  0.464   Jarque-Bera (JB):                1.326\n",
      "Skew:                           0.129   Prob(JB):                        0.515\n",
      "Kurtosis:                       3.116   Cond. No.                     6.07e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.07e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "result7 = sm.OLS.from_formula('Sales ~' + '+'.join(df2.columns.difference(['Sales'])) + ' - Population - Education - Age - Urban - US + Income:Advertising + Price:Age', df2).fit()\n",
    "print(result7.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Beat the teacher</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.875\n",
      "Model:                            OLS   Adj. R-squared:                  0.872\n",
      "Method:                 Least Squares   F-statistic:                     302.7\n",
      "Date:                Wed, 26 Feb 2020   Prob (F-statistic):          6.46e-170\n",
      "Time:                        18:54:19   Log-Likelihood:                -566.81\n",
      "No. Observations:                 400   AIC:                             1154.\n",
      "Df Residuals:                     390   BIC:                             1194.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               6.4243      0.965      6.660      0.000       4.528       8.321\n",
      "ShelveLoc[T.Good]       4.8327      0.152     31.734      0.000       4.533       5.132\n",
      "ShelveLoc[T.Medium]     1.9461      0.125     15.559      0.000       1.700       2.192\n",
      "Advertising             0.0638      0.021      3.061      0.002       0.023       0.105\n",
      "CompPrice               0.0928      0.004     22.678      0.000       0.085       0.101\n",
      "Income                  0.0109      0.003      4.210      0.000       0.006       0.016\n",
      "Price                  -0.1011      0.007    -13.664      0.000      -0.116      -0.087\n",
      "Income:Advertising      0.0008      0.000      2.724      0.007       0.000       0.001\n",
      "Age                    -0.0587      0.016     -3.706      0.000      -0.090      -0.028\n",
      "Price:Age               0.0001      0.000      0.853      0.394      -0.000       0.000\n",
      "==============================================================================\n",
      "Omnibus:                        1.056   Durbin-Watson:                   2.020\n",
      "Prob(Omnibus):                  0.590   Jarque-Bera (JB):                1.028\n",
      "Skew:                           0.124   Prob(JB):                        0.598\n",
      "Kurtosis:                       2.977   Cond. No.                     1.25e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.25e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "beat_teacher2 = sm.OLS.from_formula('Sales ~' + '+'.join(df2.columns.difference(['Sales'])) + ' - Population - Education - Age - Urban - US + Income*Advertising + Price*Age', df2).fit()\n",
    "print(beat_teacher2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interpretation</h3>\n",
    "\n",
    "The increase of R2 (from teacher's version of 87.0\\% to this version 87.5\\%) suggests that it leads to an improved model. Furthermore it is visible, that the degree of freedom increased by one (from 8 to 9 - indicating that the more complex model has one additional parameters), and very small p-value. Higher degrees of freedom generally mean larger sample sizes, a higher degree of freedom means more power to reject a false null hypothesis and find a significant result. Thus, the conclusion is, that the change to 'Income * Advertising + Price * Age' to the beat_teacher2 fit is superior to the result7 fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
